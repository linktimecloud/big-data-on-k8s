## ------------------------------------------------------------------------------
## zookeeper:
## ------------------------------------------------------------------------------
zookeeper:
  ## Configure Zookeeper resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: ~

  ## The JVM heap size to allocate to Zookeeper
  env:
    ZK_HEAP_SIZE: 1G

  ## The number of zookeeper server to have in the quorum.
  replicaCount: 3

## ------------------------------------------------------------------------------
## hdfs-config-k8s:
## ------------------------------------------------------------------------------
hdfs-config-k8s:
  ## Custom hadoop config keys passed to the hdfs configmap as extra keys.
  customHadoopConfig:
     coreSite: {
       hadoop.tmp.dir: "/usr/local/hadoop/tmp",
       hadoop.proxyuser.ec2-user.groups: "*",
       hadoop.proxyuser.ec2-user.hosts: "*",
       hadoop.proxyuser.root.groups: "*",
       hadoop.proxyuser.root.hosts: "*",
       hadoop.proxyuser.sqoop.groups: "*",
       hadoop.proxyuser.sqoop.hosts: "*",
       hadoop.proxyuser.hue.groups: "*",
       hadoop.proxyuser.hue.hosts: "*",
       hadoop.proxyuser.zeppelin.groups: "*",
       hadoop.proxyuser.zeppelin.hosts: "*",
       hadoop.proxyuser.hive.groups: "*",
       hadoop.proxyuser.hive.hosts: "*",
       hadoop.proxyuser.httpfs.groups: "*",
       hadoop.proxyuser.httpfs.hosts: "*",
       hadoop.proxyuser.livy.groups: "*",
       hadoop.proxyuser.livy.hosts: "*",
       hadoop.proxyuser.dcos.groups: "*",
       hadoop.proxyuser.dcos.hosts: "*"
     }
      ## Set config key and value pairs, e.g.
      # hadoop.http.authentication.type: kerberos

     hdfsSite: {}
      ## Set config key and value pairs, e.g.
      # dfs.datanode.use.datanode.hostname: "false"

## ------------------------------------------------------------------------------
## hdfs-journalnode-k8s:
## ------------------------------------------------------------------------------
hdfs-journalnode-k8s:
  persistence:
    ## Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"
    ## To choose a suitable persistent volume from available static volumes, selectors
    ## are used.
    # selector:
    #   matchLabels:
    #     volume-type: hdfs-ssd
    accessMode: ReadWriteOnce
    size: 1Gi

  ## Node labels and tolerations for pod assignment
  nodeSelector: {}
  tolerations: []
  affinity: {}

  resources:
    requests:
      cpu: "0.2"
      memory: "256Mi"
    limits:
      cpu: "1.0"
      memory: "1024Mi"
  
  ## Array of snippets with your sidecar containers
  sidecars: []
  extraVolumes: []
  extraVolumeMounts: []

  ## Probes
  livenessProbe:
    tcpSocket:
      port: 8485
    periodSeconds: 30
    initialDelaySeconds: 15
  
  readinessProbe: {}
  startupProbe: {}

## ------------------------------------------------------------------------------
## hdfs-namenode-k8s:
## ------------------------------------------------------------------------------
hdfs-namenode-k8s:
  ## Name of the namenode start script in the config map.
  namenodeStartScript: format-and-run.sh

  ## A namenode start script that can have user specified content.
  ## Can be used to conduct ad-hoc operation as specified by a user.
  ## To use this, also set the namenodeStartScript variable above
  ## to custom-run.sh.
  customRunScript: |
    #!/bin/bash -x
    echo Write your own script content!
    echo This message will disappear in 10 seconds.
    sleep 10

  persistence:
    ## Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

    ## To choose a suitable persistent volume from available static volumes, selectors
    ## are used.
    # selector:
    #   matchLabels:
    #     volume-type: hdfs-ssd

    accessMode: ReadWriteOnce

    size: 10Gi

  ## Whether or not to use hostNetwork in namenode pods. Disabling this will break
  ## data locality as namenode will see pod virtual IPs and fails to equate them with
  ## cluster node physical IPs associated with data nodes.
  ## We currently disable this only for CI on minikube.
  # hostNetworkEnabled: true

  ## Node labels and tolerations for pod assignment
  nodeSelector: {}
  tolerations: []
  affinity: {}

# jmx port
  jmxPort: 54321

  resources:
    requests:
      cpu: "0.2"
      memory: "256Mi"
    limits:
      cpu: "1.0"
      memory: "2048Mi"
  
  heapSize: 512
  
  ## Array of snippets with your sidecar containers
  sidecars: []
  extraVolumes: []
  extraVolumeMounts: []

  ## Probes
  livenessProbe:
    exec:
      command:
        - /nn-scripts/health-check.sh
    periodSeconds: 30
    initialDelaySeconds: 15
    timeoutSeconds: 30

  startupProbe:
    exec:
      command:
        - /nn-scripts/health-check.sh
    initialDelaySeconds: 30
    periodSeconds: 15
    timeoutSeconds: 30
    failureThreshold: 8
  
  readinessProbe: {}

## ------------------------------------------------------------------------------
## hdfs-simple-namenode-k8s:
## ------------------------------------------------------------------------------
hdfs-simple-namenode-k8s:
  ## Path of the local disk directory on a cluster node that will contain the namenode
  ## fsimage and edit logs. This will be mounted to the namenode as a k8s HostPath
  ## volume.
  nameNodeHostPath: /hdfs-name

  ## Node labels and tolerations for pod assignment
  nodeSelector: {}
  tolerations: []
  affinity: {}

## ------------------------------------------------------------------------------
## hdfs-datanode-k8s:
## ------------------------------------------------------------------------------
hdfs-datanode-k8s:
persistence:
    volumesCount: 1
    accessMode: ReadWriteOnce
    size: 10Gi

    ## Node labels and tolerations for pod assignment
  nodeSelector: {}
  tolerations: []
  affinity: {}

  # jmx port
  jmxPort: 54322

  replicas: 3

  resources:
    requests:
      cpu: "0.2"
      memory: "256Mi"
    limits:
      cpu: "1.0"
      memory: "1024Mi"
  
  ## Array of snippets with your sidecar containers
  sidecars: []
  extraVolumes: []
  extraVolumeMounts: []

  ## Probes
  livenessProbe:
    exec:
      command:
        - /dn-scripts/check-status.sh
    initialDelaySeconds: 60
    periodSeconds: 60
    timeoutSeconds: 30
  
  readinessProbe: {}
  startupProbe: {}
## ------------------------------------------------------------------------------
## hdfs-krb5-k8s:
## ------------------------------------------------------------------------------
hdfs-krb5-k8s:
  persistence:
    ## Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

    ## To choose a suitable persistent volume from available static volumes, selectors
    ## are used.
    # selector:
    #   matchLabels:
    #     volume-type: hdfs-ssd

    accessMode: ReadWriteOnce

    size: 20Gi

  ## We use a 3rd party image built from https://github.com/gcavalcante8808/docker-krb5-server.
  ## TODO: The pod currently prints out the admin account in plain text.
  ## Supply an admin account password using a k8s secret.
  ## TODO: The auto-generated passwords might be weak due to low entropy.
  ## Increase entropy by running rngd or haveged.
  ## TODO: Using latest tag is not desirable. The current image does not have specific tags.
  ## Find a way to fix it.
  image:
    repository: gcavalcante8808/krb5-server

    tag: latest

    pullPolicy: IfNotPresent

  service:
    type: ClusterIP

    port: 88
## ------------------------------------------------------------------------------
## Global values affecting all sub-charts:
## ------------------------------------------------------------------------------
global:
  hostNetworkEnabled: false
  ## A list of the local disk directories on cluster nodes that will contain the datanode
  ## blocks. These paths will be mounted to the datanode as K8s HostPath volumes.
  ## In a command line, the list should be enclosed in '{' and '}'.
  ## e.g. --set "dataNodeHostPath={/hdfs-data,/hdfs-data1}"
  dataNodeHostPath:
    - /hdfs-data

  ## Parameters for determining which Unix user and group IDs to use in pods.
  ## Persistent volume permission may need to match these.
  podSecurityContext:
    enabled: false
    runAsUser: 0
    fsGroup: 1000

  ## Whether or not to expect namenodes in the HA setup.
  namenodeHAEnabled: false

  ## The number of zookeeper server to have in the quorum.
  ## This should match zookeeper.replicaCount above. Used only when
  ## namenodeHAEnabled is set.
  zookeeperQuorumSize: 3

  ## Override zookeeper quorum address. Zookeeper is used for determining which namenode
  ## instance is active. Separated by the comma character. Used only when
  ## namenodeHAEnabled is set.
  ##
  # zookeeperQuorumOverride: zk-0.zk-svc.default.svc.cluster.local:2181,zk-1.zk-svc.default.svc.cluster.local:2181,zk-2.zk-svc.default.svc.cluster.local:2181

  zookeeperParentZnode: /hadoop-ha/hdfs-k8s
  ## How many journal nodes to launch as a quorum. Used only when
  ## namenodeHAEnabled is set.
  journalnodeQuorumSize: 1

  ## Whether or not to enable default affinity setting.
  defaultAffinityEnabled: true

  ## Whether or not Kerberos support is enabled.
  kerberosEnabled: false

  ## Override th name of the hdfs ConfigMap
  ## containing the *-site.xml files.
  # hdfsConfigMapOverride: hdfs-config

  ## Effective only if Kerberos is enabled. Override th name of the k8s
  ## ConfigMap containing the kerberos config file.
  ##
  # kerberosConfigMapOverride: kerberos-config

  ## Effective only if Kerberos is enabled. Name of the kerberos config file inside
  ## the config map.
  kerberosConfigFileName: krb5.conf

  ## Effective only if Kerberos is enabled. Override the name of the k8s Secret
  ## containing the kerberos keytab files of per-host HDFS principals.
  ## The secret should have multiple data items. Each data item name
  ## should be formatted as:
  ##    `HOST-NAME.keytab`
  ## where HOST-NAME should match the cluster node
  ## host name that each per-host hdfs principal is associated with.
  ##
  # kerberosKeytabsSecretOverride: hdfs-kerberos-keytabs

  ## Required to be non-empty if Kerberos is enabled. Specify your Kerberos realm name.
  ## This should match the realm name in your Kerberos config file.
  kerberosRealm: LINKTIME.CLOUD

  ## Effective only if Kerberos is enabled. Enable protection of datanodes using
  ## the jsvc utility. See the reference doc at
  ## https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/SecureMode.html#Secure_DataNode
  jsvcEnabled: false

  httpsEnables: false
  ## Docker registry
  dockerRegistry: ltc-registry.cn-hangzhou.cr.aliyuncs.com/linktimecloud
  imageTag: 3.1.1-1.0.0

  ## rbac
  rbac:
    create: false
    serviceAccountName: hdfs
    roleName: hdfs
    roleBindingName: hdfs-role-binding

## Tags and conditions for triggering a group of relevant subcharts.
tags:
  ## Trigger all subcharts required for high availability. Enabled by default.
  ha: true

  ## Trigger all subcharts required for using Kerberos. Disabled by default.
  kerberos: true

  ## Trigger all subcharts required for non-HA setup. Disabled by default.
  simple: false

